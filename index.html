<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Yuye (Julia) Zhou - Academic Portfolio</title>
<link rel="stylesheet" href="style.css">
</head>
<body>


<header>
<img src="Profile.jpg" alt="Yuye Zhou">
<h1>Yuye (Julia) Zhou</h1>
<h2>Ph.D. Student · UC Berkeley · Geospatial Science & Environmental Planning</h2>
<nav>
<a href="https://ced.berkeley.edu/people/yuye-julia-zhou">Official Berkeley Page</a>
<a href="https://scholar.google.com/citations?user=bRPBrfYAAAAJ&hl=en&oi=ao">Google Scholar</a>
<a href="https://www.linkedin.com/in/yuyezhou/">LinkedIn</a>
<a href="mailto:yuyezhou@berkeley.edu">Email</a>
</nav>
</header>


<section>
<h2>Summary</h2>
<div class="card">
<p>Ph.D. researcher at UC Berkeley specializing in <strong>LiDAR, remote sensing, and multimodal perception</strong>. Experienced in architecting scalable pipelines for large-scale spatial data processing (LiDAR, camera, satellite) and applying causal machine learning to draw robust statistical conclusions. Passionate about advancing <strong>sensing and perception systems for autonomous vehicles</strong> through rigorous data analysis and systems-level requirement derivation.</p>
</div>
</section>


<section>
<h2>Education</h2>
<div class="card">
<p><strong>University of California, Berkeley</strong> — Ph.D. Student in Geospatial Science & Environmental Planning (2024–Present)<br>
-- Research interests: Geospatial AI, multimodal perception for AV-ready streets.<br>
-- Advisor: <a href="https://ced.berkeley.edu/people/lu-liang">Prof. Lu Liang</a>, <a href="https://sites.google.com/site/liang3mlab/home">3M Lab</a></p>


<p><strong>Nanjing University</strong> — M.S. in GIS & Urban Planning (2021–2024), GPA: 3.93/4.0</p>


<p><strong>Zhejiang University</strong> — B.Eng. in Urban Planning (Highest Distinction), Minor in Law (2016–2021), GPA: 3.87/4.0</p>
</div>
</section>


<section>
<h2>Technical Skills</h2>
<div class="card">
<p><strong>Programming:</strong> Python (pandas, geopandas, PyTorch, scikit-learn), SQL, C++, MATLAB, R</p>
<p><strong>Geospatial/Sensing:</strong> ArcGIS Pro, QGIS, LiDAR (PDAL, SLAM), Google Earth Engine</p>
<p><strong>ML/AI:</strong> Multimodal Fusion, AI Agents, LLMs, CNN, GNN, Causal ML (Double ML, DeepIV), XAI</p>
<p><strong>Tools:</strong> Git, Tableau, Adobe Illustrator, LaTeX, Photoshop</p>
</div>
</section>


<section>
<h2>Research & Projects</h2>
<div class="card">
<p><strong>Multimodal Urban Perception for Autonomous Systems</strong> (UC Berkeley, Jan 2025– )<br>
Led the development of a causal graph fusion framework integrating LiDAR, panoramic camera imagery, and satellite data to ensure robust perception in extreme urban conditions.</p>
<ul>
<li>Engineered a joint evaluation protocol for dynamic robustness and static scene understanding (Sky View Factor, occlusion).</li>
<li>Generated interpretable modality attributions to enhance system reliability in safety-critical scenarios.</li>
</ul>
</div>


<div class="card">
<p><strong>Large-Scale LiDAR Processing for AV Line-of-Sight Analysis</strong> (UC Berkeley, Oct 2024– )<br>
Built scalable Python pipeline to process 900+ LiDAR (LAZ) files for San Francisco, extending to 10+ US cities.</p>
<ul>
<li>Developed algorithms to compute Sky View Factor (SVF) and quantify occlusions from buildings and trees.</li>
<li>Produced high-resolution DSM/DEMs to inform AV sensor validation.</li>
</ul>
</div>


<div class="card">
<p><strong>Multimodal AI-Agent Framework for Urban Simulation</strong> (UC Berkeley, Feb 2025– )<br>
Led development of AI-agent framework to simulate human-environment interactions at city scale.</p>
